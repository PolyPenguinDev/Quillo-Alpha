#!/usr/bin/env python
import argparse
import requests
import markdownify
from bs4 import BeautifulSoup
import os
import re
import json
import html2text
h = html2text.HTML2Text()
bolde = {"A":"ğ—”","B":"ğ—•","C":"ğ—–","D":"ğ——","E":"ğ—˜","F":"ğ—™","G":"ğ—š", "H":"ğ—›", "I":"ğ—œ","J":"ğ—","K":"ğ—","L":"ğ—Ÿ","M":"ğ— ","N":"ğ—¡","O":"ğ—¢","P":"ğ—£","Q":"ğ—¤","R":"ğ—¥","S":"ğ—¦","T":"ğ—§","U":"ğ—¨","V":"ğ—©","W":"ğ—ª","X":"ğ—«","Y":"ğ—¬","Z":"ğ—­","a":"ğ—®","b":"ğ—¯","c":"ğ—°","d":"ğ—±","e":"ğ—²","f":"ğ—³","g":"ğ—´","h":"ğ—µ","i":"ğ—¶","j":"ğ—·","k":"ğ—¸","l":"ğ—¹","m":"ğ—º","n":"ğ—»","o":"ğ—¼","p":"ğ—½","q":"ğ—¾","r":"ğ—¿","s":"ğ˜€","t":"ğ˜","u":"ğ˜‚","v":"ğ˜ƒ","w":"ğ˜„","x":"ğ˜…","y":"ğ˜†","z":"ğ˜‡","1":"ğŸ­","2":"ğŸ®","3":"ğŸ¯","4":"ğŸ°","5":"ğŸ±","6":"ğŸ²","7":"ğŸ³","8":"ğŸ´","9":"ğŸµ", "0":"ğŸ¬"}
h.ignore_links = False
def bold(text):
    line = ""
    for i in text.split("\n"):
        ins = 0
        isbold=True
        linez = True
        t=""
        for j in i:
            if isbold:
                if ins == 2:
                    if j != "#":
                        linez= False
                elif ins <= 1:
                    isbold = j == "#"
                    linez = linez and isbold
                    if j != "#":
                        t+=j
                else:
                    try:
                        t+=bolde[j]
                    except:
                        t+=j
            else:
                t+=j
            ins +=1
        if line != "":
            line += "\n"+t
            if linez and len(t) > 2:
                line += "\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

        else:
            line = t
    return line
        

def replace_bullets_with_symbol(markdown_text, symbol='â€¢'):
    pattern = r'^\s*[-*+]'
    return re.sub(pattern, symbol, markdown_text, flags=re.MULTILINE)

def replace_links(markdown):
    link_pattern = re.compile(r'\[([^]]+)\]\(([^)]+)\)')
    matches = link_pattern.findall(markdown)
    link_list = [[] for _ in range(len(matches))]

    for i, (text, link) in enumerate(matches, start=1):
        reference = f'{text} [{i}]'
        link_list[i-1].append(link)
        markdown = link_pattern.sub(reference, markdown, 1)

    return markdown, link_list

def remove_tags(html):
    soup = BeautifulSoup(html, "html.parser")
    for data in soup(['style', 'script', 'img']):
        data.decompose()
    return soup.prettify()

def clear_console():
    os.system('cls' if os.name == 'nt' else 'clear')

def main():
    clear_console()
    parser = argparse.ArgumentParser(description="A simple command-line program.")
    parser.add_argument("site", help="The website to view.")
    parser.add_argument("-o", "--open", action="store_true", help="Opens a website.")
    parser.add_argument("-v", "--view", action="store_true", help="Formats HTML into a more readable format")
    parser.add_argument("-g", "--go", action="store_true", help="Opens a link that is on the webpage")
    parser.add_argument("-s", "--search", action="store_true", help="Searches something on google")

    args = parser.parse_args()

    if args.open:
        try:
            int(args.site)
            with open('tabs.json', 'r') as f:
                l = json.load(f)
            args.site = l["tabs"][l["current"]["links"]][int(args.site)-1][0]
        except:
            args.site = f"http://www.{args.site}" if not args.site.startswith('http') else args.site
        print(f"Loading {args.site} ...")
        response = requests.get(args.site)
        soup = BeautifulSoup(response.content, 'html.parser')
        out = remove_tags(soup.prettify())
        if args.view:
            title = soup.title.text.strip()
            clear_console()
            print(f"\n---------âƒğŸ–‹ï¸ Quillo Text-Based Browser---------")
            for i in range(len(f"| 1. {title} |")):
                print("â”€", end = "")
            print()
            print("|", end = "")
            with open('tabs.json', 'r') as f:
                l = json.load(f)
            n = 0
            s = "|"
            for i in l["tabs"]:
                n+=1
                s+=" "+str(n)+". "+i["title"]+" |"
            for i in range(len(s)):
                    print("â”€", end = "")
            print()
            print(s)
            for i in range(len(s)):
                    print("â”€", end = "")
            print("\n")
            job_elements = soup.find_all()
            out = replace_bullets_with_symbol(h.handle(out))
            print(bold(replace_links(out)[0]))
            with open('links.json', 'w') as f:
                json.dump(replace_links(out)[1], f)
    elif args.search:
        args.site = f"http://www.google.com/search?q={args.site}"
        print(f"Loading {args.site} ...")
        response = requests.get(args.site)
        soup = BeautifulSoup(response.content, 'html.parser')
        title = soup.title.text.strip()
        clear_console()
        print(f"\n------âƒğŸ–‹ï¸ Quillo Text-Based Browser -----\n")
        print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print(f"| 1. {title} |")
        print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print("""
   ___                  _     
  /  _|  ___  ___  __ _| |___ 
 â–•  (| |/ _ \/ _ \/ _` | / -_)
  \____|\___/\___/\__, |_\___|
                  |___/      
              """)
        rso_div = soup.find('div', id='main')
        nested_divs = rso_div.find_all('div')
        links = []
        for nested_div in nested_divs:
            a_tag = nested_div.find('a')
            if a_tag:
                try:
                    link_text = a_tag.get("href")
                    soupp = BeautifulSoup(a_tag.prettify(), 'html.parser')
                    text_content = soupp.find('h3', class_='zBAuLc l97dzf').div.text
                    print(f"{text_content} [{str(len(links)+1)}]")
                    links.append(["https://www.google.com/"+link_text])
                except:
                    pass
        with open('links.json', 'w') as f:
            json.dump(links, f)

if __name__ == "__main__":
    main()
